{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tempfile\n",
    "import requests\n",
    "from IPython.display import Audio, clear_output\n",
    "from elevenlabs import generate, play, set_api_key, voices, Models\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "eleven_api_key = os.environ[\"ELEVEN_API_KEY\"]\n",
    "\n",
    "# Configuro GPT-4 Y Text-to-Speech API\n",
    "openai.api_key = openai_api_key\n",
    "set_api_key(eleven_api_key)\n",
    "\n",
    "voice_list = voices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciona la voy a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b5100b130f4f0eb05466bc53797b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Seleccione una voz:', options=('premade voice: Rachel', 'premade voice: Domi', 'premade …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "voice_labels = [voice.category + \" voice: \" + voice.name for voice in voice_list]\n",
    "\n",
    "voice_id_dropdown = widgets.Dropdown(\n",
    "    options=voice_labels,\n",
    "    value=voice_labels[0],\n",
    "    description=\"Seleccione una voz:\",\n",
    ")\n",
    "\n",
    "display(voice_id_dropdown)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digo la pregunta que tiene que resolver GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabar el audio con el microfono con pyaudio de la computadora para usarlo en OpenAI\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "FILE_PATH = \"audios/output.wav\"\n",
    "\n",
    "def record_audio(file_path, format, channels, rate, chunk, record_seconds):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=format, \n",
    "                    channels=channels, \n",
    "                    rate=rate, \n",
    "                    input=True, \n",
    "                    frames_per_buffer=chunk)\n",
    "\n",
    "    print(\"Comenzando la grabación...\")\n",
    "\n",
    "    frames = []\n",
    "    for i in range(0, int(rate / chunk * record_seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Fin de la grabación.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(file_path, \"wb\")\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(format))\n",
    "    wf.setframerate(rate)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "    wf.close()\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        audio_file = open(file_path, \"rb\")\n",
    "        transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "        return transcript\n",
    "    else:\n",
    "        print(\"El archivo no existe\")\n",
    "        return None\n",
    "\n",
    "def obtener_texto_de_microfono():\n",
    "    record_seconds = 4  # puedes cambiar este valor a la duración deseada\n",
    "    record_audio(FILE_PATH, FORMAT, CHANNELS, RATE, CHUNK, record_seconds)\n",
    "    transcript = transcribe_audio(FILE_PATH)\n",
    "    if transcript:\n",
    "        print(transcript)\n",
    "    return transcript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración e interacción con ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando la grabación...\n",
      "Fin de la grabación.\n",
      "{\n",
      "  \"text\": \"Salir del programa.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from mutagen.mp3 import MP3\n",
    "import IPython.display as ipd\n",
    "import time\n",
    "\n",
    "chatgpt_model = \"gpt-3.5-turbo\"\n",
    "chatgpt_system = \"Eres asistente muy util para conversar. Tus respuestas no deben ser largas.\"\n",
    "\n",
    "# Encuentra el índice de la opción seleccionada\n",
    "selected_voice_index = voice_labels.index(voice_id_dropdown.value)\n",
    "selected_voice_id    = voice_list[selected_voice_index].voice_id\n",
    "\n",
    "# Function para obtener la respuesta GPT-4\n",
    "def get_gpt_response(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=chatgpt_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": chatgpt_system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Función principal para interactuar con GPT\n",
    "def interact_with_gpt(prompt):\n",
    "    response_text = get_gpt_response(prompt)\n",
    "\n",
    "    import requests\n",
    "\n",
    "    CHUNK_SIZE = 1024\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/\" + selected_voice_id\n",
    "\n",
    "    headers = {\n",
    "      \"Accept\": \"audio/mpeg\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"xi-api-key\": eleven_api_key\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "      \"text\": response_text,\n",
    "      \"model_id\" : \"eleven_multilingual_v1\",\n",
    "      \"voice_settings\": {\n",
    "        \"stability\": 0.4,\n",
    "        \"similarity_boost\": 1.0\n",
    "      }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    \n",
    "    # Salva el audio en un archivo temporal\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "        f.flush()\n",
    "        temp_filename = f.name\n",
    "\n",
    "    return temp_filename\n",
    "\n",
    "#response_audio_file = interact_with_gpt(transcript.text)\n",
    "# autoplay audio\n",
    "#play(response_audio_file, notebook=True)\n",
    "\n",
    "# Función para interactuar continuamente con GPT\n",
    "def continuous_interaction():\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        prompt = obtener_texto_de_microfono()\n",
    "        if 'salir' in prompt.text.lower():\n",
    "            break\n",
    "        audio_file = interact_with_gpt(prompt.text)\n",
    "        # Calcula la duración del audio\n",
    "        audio = MP3(audio_file)\n",
    "        duration = audio.info.length\n",
    "        # Reproduce el audio\n",
    "        ipd.display(ipd.Audio(audio_file, autoplay=True))\n",
    "        # Pausa la ejecución del programa por la duración del audio\n",
    "        time.sleep(duration + 5)\n",
    "\n",
    "# Example usage\n",
    "continuous_interaction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
