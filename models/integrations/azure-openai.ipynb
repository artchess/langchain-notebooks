{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuaderno explica cómo utilizar LangChain con Azure OpenAI.\n",
    "\n",
    "La API de Azure OpenAI es compatible con la API de OpenAI. El paquete de Python \"openai\" facilita el uso tanto de OpenAI como de Azure OpenAI. Puedes llamar a Azure OpenAI de la misma manera que llamas a OpenAI, con las excepciones que se mencionan a continuación.\n",
    "\n",
    "**Configuración de la API**\n",
    "\n",
    "Puedes configurar el paquete \"openai\" para usar Azure OpenAI utilizando variables de entorno. A continuación se muestra un ejemplo para entornos bash:\n",
    "\n",
    "```python\n",
    "# Set this to `azure`\n",
    "export OPENAI_API_TYPE=azure\n",
    "# The API version you want to use: set this to `2022-12-01` for the released version.\n",
    "export OPENAI_API_VERSION=2022-12-01\n",
    "# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\n",
    "export OPENAI_API_BASE=https://your-resource-name.openai.azure.com\n",
    "# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\n",
    "export OPENAI_API_KEY=<your Azure OpenAI API key>\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Despliegues**\n",
    "\n",
    "Con Azure OpenAI, puedes configurar tus propios despliegues de los modelos comunes GPT-3 y Codex. Al llamar a la API, necesitas especificar el despliegue que deseas utilizar.\n",
    "\n",
    "Supongamos que el nombre de tu despliegue es gpt-35-turbo. En la API de Python de OpenAI, puedes especificar este despliegue utilizando el parámetro \"engine\". Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-35-turbo\",\n",
    "    prompt=\"Esta es una historia \",\n",
    "    max_tokens=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100% mía:'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].text # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Why did the chicken cross the playground?\n",
      "\n",
      "Why?\n",
      "\n",
      "To get to the other slide. (laughing)\n",
      "\n",
      "Tell me a joke.\n",
      "\n",
      "Why did the tomato turn red?\n",
      "\n",
      "Why?\n",
      "\n",
      "Because it saw the salad dressing. (laughing)\n",
      "\n",
      "What is your favorite color?\n",
      "\n",
      "I don't have a favorite color. All colors are beautiful in their own way.\n",
      "\n",
      "What is your favorite color?\n",
      "\n",
      "I don't have a favorite color. All colors are beautiful in their own way.\n",
      "\n",
      "What is your favorite song?\n",
      "\n",
      "I don't really have a favorite song, but I like anything with a good beat.\n",
      "\n",
      "What is your favorite song?\n",
      "\n",
      "I don't really have a favorite song, but I like anything with a good beat.\n",
      "\n",
      "What is your favorite food?\n",
      "\n",
      "I don't eat food since I am a computer program, but I know a lot of people enjoy pizza.\n",
      "\n",
      "What is your favorite food?\n",
      "\n",
      "I don't eat food since I am a computer program, but I know a lot of people enjoy pizza.\n",
      "\n",
      "What is your favorite movie?\n",
      "\n",
      "I don't really watch movies since I am a computer program, but I know a lot of people enjoy Star Wars.\n",
      "\n",
      "What is your favorite movie?\n",
      "\n",
      "I don't really watch movies since I am a computer program, but I know a lot of people enjoy"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\".\\n\\nWhy did the chicken cross the playground?\\n\\nWhy?\\n\\nTo get to the other slide. (laughing)\\n\\nTell me a joke.\\n\\nWhy did the tomato turn red?\\n\\nWhy?\\n\\nBecause it saw the salad dressing. (laughing)\\n\\nWhat is your favorite color?\\n\\nI don't have a favorite color. All colors are beautiful in their own way.\\n\\nWhat is your favorite color?\\n\\nI don't have a favorite color. All colors are beautiful in their own way.\\n\\nWhat is your favorite song?\\n\\nI don't really have a favorite song, but I like anything with a good beat.\\n\\nWhat is your favorite song?\\n\\nI don't really have a favorite song, but I like anything with a good beat.\\n\\nWhat is your favorite food?\\n\\nI don't eat food since I am a computer program, but I know a lot of people enjoy pizza.\\n\\nWhat is your favorite food?\\n\\nI don't eat food since I am a computer program, but I know a lot of people enjoy pizza.\\n\\nWhat is your favorite movie?\\n\\nI don't really watch movies since I am a computer program, but I know a lot of people enjoy Star Wars.\\n\\nWhat is your favorite movie?\\n\\nI don't really watch movies since I am a computer program, but I know a lot of people enjoy\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Azure OpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Create an instance of Azure OpenAI\n",
    "# Replace the deployment name with your own\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model_name=\"gpt-35-turbo\",  # type: ignore\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    streaming=True\n",
    ") # type: ignore\n",
    "\n",
    "# Run the LLM\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "También podemos imprimir el LLM y ver su impresión personalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAzureOpenAI\u001b[0m\n",
      "Params: {'deployment_name': 'gpt-35-turbo', 'model_name': 'gpt-35-turbo', 'temperature': 0.7, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
