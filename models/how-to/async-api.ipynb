{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LangChain proporciona soporte asincrónico para LLMs aprovechando la biblioteca asyncio.\n",
    "\n",
    "El soporte asincrónico es especialmente útil para llamar a múltiples LLMs de manera concurrente, ya que estas llamadas están limitadas por la red. Actualmente, se admiten OpenAI, PromptLayerOpenAI, ChatOpenAI y Anthropic, pero el soporte asincrónico para otros LLMs está en el plan de desarrollo.\n",
    "\n",
    "Puedes utilizar el método \"agenerate\" para llamar de forma asincrónica a un LLM de OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, how about you?\n",
      "\n",
      "\n",
      "I'm doing great. How about you?\n",
      "\n",
      "\n",
      "I'm doing well. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.acompletion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you! How are you?\n",
      "\u001b[1mConcurrent executed in 36.74 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about yourself?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about yourself?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\u001b[1mSerial executed in 28.64 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import asyncio\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9) # type: ignore\n",
    "    for _ in range(10):\n",
    "        resp = llm.generate([\"Hello, how are you?\"])\n",
    "        print(resp.generations[0][0].text)\n",
    "\n",
    "\n",
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"Hello, how are you?\"])\n",
    "    print(resp.generations[0][0].text)\n",
    "\n",
    "\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9) # type: ignore\n",
    "    tasks = [async_generate(llm) for _ in range(10)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "s = time.perf_counter()\n",
    "# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n",
    "await generate_concurrently() \n",
    "elapsed = time.perf_counter() - s\n",
    "print('\\033[1m' + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + '\\033[0m')\n",
    "\n",
    "s = time.perf_counter()\n",
    "generate_serially()\n",
    "elapsed = time.perf_counter() - s\n",
    "print('\\033[1m' + f\"Serial executed in {elapsed:0.2f} seconds.\" + '\\033[0m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
