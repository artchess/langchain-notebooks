{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el wrapper del LLM para OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importo el wrapper del LLM de openAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompts**: Se ejecuta una llamada sencilla al LLM de OpenAI para probar que funciona correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Calcetines Coloridos S.A.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9) # mayor temperatura hace que la respuesta sea mas creativa o random\n",
    "\n",
    "text = \"¿Cuál sería un buen nombre de una compañia en español para una empresa que hace calcetines de colores?\"\n",
    "respuesta = llm(text)\n",
    "print(respuesta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente no enviamos inputs de usuario directamente al LLM. En lugar de eso probablemente tomaríamos el imput del usuario y contruiríamos un prompt, para luego mandarlo al LLM, como en el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cuál es un buen nombre en español para una compañía que hace calcetines de colores?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"producto\"],\n",
    "    template=\"¿Cuál es un buen nombre en español para una compañía que hace {producto}?\",\n",
    ")\n",
    "\n",
    "print(prompt.format(producto=\"calcetines de colores\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chains (Candenas)**: Combinar LLMs y Prompts en workflows de multiples pasos. Una verdadera aplicación no es solamente un solo elemento primitivo (LLMs o Prompts) sino una combinación de varios. Una cadena en LangChain esta formado de links, que pueden ser tanto primitivas como LLMs u otras cadenas. \n",
    "\n",
    "La cadena más central o más básica es una LLMChain, que consiste en un PromptTemplate y un LLM. \n",
    "\n",
    "Podemos contruir un LLMChain que toma un input de usuario, lo formatea en un PromptTemplate y luego pasa la respuesta formateada a un LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m¿Cuál es un buen nombre en español para una compañía que hace calcetines de rayas?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Calcetines Rayados S.A.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"producto\"],\n",
    "    template=\"¿Cuál es un buen nombre en español para una compañía que hace {producto}?\",\n",
    ")\n",
    "\n",
    "llm2 = OpenAI(temperature=0.1)\n",
    "\n",
    "chain = LLMChain(llm=llm2, prompt=prompt2, verbose=True)\n",
    "\n",
    "resultado = chain.run(\"calcetines de rayas\")\n",
    "print(resultado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agents**: sirven para que de manera dinámica se puedan llamar cadenas basandose en el input del usuario. Hasta ahora las cadenas se ejecutan en un orden predeterminado. \n",
    "Los agentes usan un LLM para determinar que acciones tomar y en que orden. Una acción puede consistir en utilizar herramienta y observar su resultado, o en volver al usuario.\n",
    "Cuando se utilizan correctamente, los agentes pueden ser extremadamente potentes. \n",
    "\n",
    "Conceptos:\n",
    "\n",
    "* Herramienta (Tool): Herramienta: Una función que realiza una tarea específica. Esto puede ser cosas como: Google Search, consulta de base de datos, Python REPL, otras cadenas. La interfaz para una herramienta es actualmente una función que se espera que tenga una cadena como entrada, con una cadena como salida.\n",
    "\n",
    "* LLM: El modelo de lenguaje que alimenta al agente.\n",
    "\n",
    "* Agente: El agente a utilizar. Debe ser una cadena que haga referencia a una clase de agente de soporte.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar el agente SerpApi se instala este paquete: pip install google-search-results se crea una cuenta aqui: https://serpapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"] = \"18b0570933955ef4cacfe91e7500ba68977c9d19a37f2b959534c3ee522c652d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out what the temperature was in Toluca yesterday\n",
      "Action: Search\n",
      "Action Input: \"temperature Toluca Estado de México yesterday Celsius\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mToluca Weather History for the Previous 24 Hours ; 82 °F · 79 °F · 77 °F ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert Fahrenheit to Celsius\n",
      "Action: Calculator\n",
      "Action Input: 82 °F\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 27.77777777777778\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 27.77777777777778 °C\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "27.77777777777778 °C\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# vamos a cargar algunas herramientas a utilizar. Ten en cuenta que la herramienta `llm-math` utiliza un LLM, así que tenemos que pasarlo.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# inicialicemos un agente con las herramientas, el modelo de lenguaje y el tipo de agente que queremos utilizar.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# probamos el agente\n",
    "respuesta = agent.run(\"¿Cuál fue la temperatura máxima en Toluca, Estado de México el dia de ayer en Celsius?\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the exchange rate and then convert 20 USD to MXN.\n",
      "Action: Search\n",
      "Action Input: USD/MXN exchange rate Mexico\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mUSDMXN:CUR. USD-MXN X-RATE ; Open. 17.5789 ; Prev Close. 17.5783 ; YTD Return. -9.79% ; Day Range. 17.578617.5938 ; 52 Week Range. 17.535321.0535.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to use the exchange rate to convert 20 USD to MXN.\n",
      "Action: Calculator\n",
      "Action Input: 20 USD * 17.5789 MXN\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 351.57800000000003\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 20 USD is equivalent to 351.578 MXN.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "20 USD is equivalent to 351.578 MXN.\n"
     ]
    }
   ],
   "source": [
    "respuesta = agent.run(\"Busca el precio de compra del dólar en méxico (USD/MXN) y luego convierte 20 dolares al precio de compra de hoy.\")\n",
    "print(respuesta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria**: Agregando estado a cadenas y agentes\n",
    "\n",
    "Hasta ahora las cadenas y agentes son sin estado. Pero a menudo, es posible que se requiera que una cadena o agente tenga algún concepto de \"memoria\" para que pueda recordar información sobre sus interacciones anteriores.\n",
    "\n",
    "El ejemplo mas claro y sencillo es el diseño de un chatbot: quieres que recuerde mensajes anteriores para que pueda utilizar su contexto para mantener una mejor conversación. Un tipo de memoria a corto plazo. De una forma más compleja, se podría imaginar una cadena/agente que recordara piezas clave de información a lo largo del tiempo: esto sería una forma de \"memoria a largo plazo\" para mas info ver [MemPrompt](https://memprompt.com/)\n",
    "\n",
    "Langchain proporciona varias cadenas especialmente creadas para este propósito. Por ejemplo ConversationChain utilizado con dos tipos de memoria distintos.\n",
    "\n",
    "Por defecto, ConversionChain tiene un tipo simple de memoria que recuerda todas las entradas/salidas anteriores y las añade al contexto que se pasa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ¡Hola!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " ¡Hola! ¿Cómo estás? Me llamo AI-1. ¿Cómo te llamas?\n"
     ]
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"¡Hola!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ¡Hola!\n",
      "AI:  ¡Hola! ¿Cómo estás? Me llamo AI-1. ¿Cómo te llamas?\n",
      "Human: ¡Estoy bien! Aquí nadamás teniendo una conversación con una IA.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " ¡Qué interesante! ¿Qué quieres saber de mí?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"¡Estoy bien! Aquí nadamás teniendo una conversación con una IA.\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat Models**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
