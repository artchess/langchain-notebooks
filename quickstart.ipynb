{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el wrapper del LLM para OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importo el wrapper del LLM de openAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompts**: Se ejecuta una llamada sencilla al LLM de OpenAI para probar que funciona correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Calcetines Coloridos S.A.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9) # mayor temperatura hace que la respuesta sea mas creativa o random\n",
    "\n",
    "text = \"¿Cuál sería un buen nombre de una compañia en español para una empresa que hace calcetines de colores?\"\n",
    "respuesta = llm(text)\n",
    "print(respuesta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente no enviamos inputs de usuario directamente al LLM. En lugar de eso probablemente tomaríamos el imput del usuario y contruiríamos un prompt, para luego mandarlo al LLM, como en el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cuál es un buen nombre en español para una compañía que hace calcetines de colores?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"producto\"],\n",
    "    template=\"¿Cuál es un buen nombre en español para una compañía que hace {producto}?\",\n",
    ")\n",
    "\n",
    "print(prompt.format(producto=\"calcetines de colores\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chains (Candenas)**: Combinar LLMs y Prompts en workflows de multiples pasos. Una verdadera aplicación no es solamente un solo elemento primitivo (LLMs o Prompts) sino una combinación de varios. Una cadena en LangChain esta formado de links, que pueden ser tanto primitivas como LLMs u otras cadenas. \n",
    "\n",
    "La cadena más central o más básica es una LLMChain, que consiste en un PromptTemplate y un LLM. \n",
    "\n",
    "Podemos contruir un LLMChain que toma un input de usuario, lo formatea en un PromptTemplate y luego pasa la respuesta formateada a un LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m¿Cuál es un buen nombre en español para una compañía que hace calcetines de rayas?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Calcetines Rayados S.A.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"producto\"],\n",
    "    template=\"¿Cuál es un buen nombre en español para una compañía que hace {producto}?\",\n",
    ")\n",
    "\n",
    "llm2 = OpenAI(temperature=0.1)\n",
    "\n",
    "chain = LLMChain(llm=llm2, prompt=prompt2, verbose=True)\n",
    "\n",
    "resultado = chain.run(\"calcetines de rayas\")\n",
    "print(resultado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agents**: sirven para que de manera dinámica se puedan llamar cadenas basandose en el input del usuario. Hasta ahora las cadenas se ejecutan en un orden predeterminado. \n",
    "Los agentes usan un LLM para determinar que acciones tomar y en que orden. Una acción puede consistir en utilizar herramienta y observar su resultado, o en volver al usuario.\n",
    "Cuando se utilizan correctamente, los agentes pueden ser extremadamente potentes. \n",
    "\n",
    "Conceptos:\n",
    "\n",
    "* Herramienta (Tool): Herramienta: Una función que realiza una tarea específica. Esto puede ser cosas como: Google Search, consulta de base de datos, Python REPL, otras cadenas. La interfaz para una herramienta es actualmente una función que se espera que tenga una cadena como entrada, con una cadena como salida.\n",
    "\n",
    "* LLM: El modelo de lenguaje que alimenta al agente.\n",
    "\n",
    "* Agente: El agente a utilizar. Debe ser una cadena que haga referencia a una clase de agente de soporte.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar el agente SerpApi se instala este paquete: pip install google-search-results se crea una cuenta aqui: https://serpapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.environ[\"SERPAPI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out what the temperature was in Toluca yesterday\n",
      "Action: Search\n",
      "Action Input: \"temperature Toluca Estado de México yesterday Celsius\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mToluca Weather History for the Previous 24 Hours ; 82 °F · 79 °F · 77 °F ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert Fahrenheit to Celsius\n",
      "Action: Calculator\n",
      "Action Input: 82 °F\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 27.77777777777778\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 27.77777777777778 °C\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "27.77777777777778 °C\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# vamos a cargar algunas herramientas a utilizar. Ten en cuenta que la herramienta `llm-math` utiliza un LLM, así que tenemos que pasarlo.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# inicialicemos un agente con las herramientas, el modelo de lenguaje y el tipo de agente que queremos utilizar.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# probamos el agente\n",
    "respuesta = agent.run(\"¿Cuál fue la temperatura máxima en Toluca, Estado de México el dia de ayer en Celsius?\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the exchange rate and then convert 20 USD to MXN.\n",
      "Action: Search\n",
      "Action Input: USD/MXN exchange rate Mexico\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mUSDMXN:CUR. USD-MXN X-RATE ; Open. 17.5789 ; Prev Close. 17.5783 ; YTD Return. -9.79% ; Day Range. 17.578617.5938 ; 52 Week Range. 17.535321.0535.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to use the exchange rate to convert 20 USD to MXN.\n",
      "Action: Calculator\n",
      "Action Input: 20 USD * 17.5789 MXN\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 351.57800000000003\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 20 USD is equivalent to 351.578 MXN.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "20 USD is equivalent to 351.578 MXN.\n"
     ]
    }
   ],
   "source": [
    "respuesta = agent.run(\"Busca el precio de compra del dólar en méxico (USD/MXN) y luego convierte 20 dolares al precio de compra de hoy.\")\n",
    "print(respuesta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memoria**: Agregando estado a cadenas y agentes\n",
    "\n",
    "Hasta ahora las cadenas y agentes son sin estado. Pero a menudo, es posible que se requiera que una cadena o agente tenga algún concepto de \"memoria\" para que pueda recordar información sobre sus interacciones anteriores.\n",
    "\n",
    "El ejemplo mas claro y sencillo es el diseño de un chatbot: quieres que recuerde mensajes anteriores para que pueda utilizar su contexto para mantener una mejor conversación. Un tipo de memoria a corto plazo. De una forma más compleja, se podría imaginar una cadena/agente que recordara piezas clave de información a lo largo del tiempo: esto sería una forma de \"memoria a largo plazo\" para mas info ver [MemPrompt](https://memprompt.com/)\n",
    "\n",
    "Langchain proporciona varias cadenas especialmente creadas para este propósito. Por ejemplo ConversationChain utilizado con dos tipos de memoria distintos.\n",
    "\n",
    "Por defecto, ConversionChain tiene un tipo simple de memoria que recuerda todas las entradas/salidas anteriores y las añade al contexto que se pasa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ¡Hola!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " ¡Hola! ¿Cómo estás? Me llamo AI-1. ¿Cómo te llamas?\n"
     ]
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"¡Hola!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ¡Hola!\n",
      "AI:  ¡Hola! ¿Cómo estás? Me llamo AI-1. ¿Cómo te llamas?\n",
      "Human: ¡Estoy bien! Aquí nadamás teniendo una conversación con una IA.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " ¡Qué interesante! ¿Qué quieres saber de mí?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"¡Estoy bien! Aquí nadamás teniendo una conversación con una IA.\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat Models**\n",
    "\n",
    "Los Modelos de Chat son variaciones de los modelos de lenguale. Mientras que los modelos de chat usan modelos de lenguaje por debajo, la interfaz que exponen es un poco distinta: en lugar de exponer texto de entrada y texto de salida, exponen una interface donde los \"mensajes de chat\" son los inputs y outputs.\n",
    "\n",
    "Los tipos de mensajes actualmente soportados en LangChain son AIMessage, HumanMessage, SystemMessage y ChatMessage - ChatMessage toma un parámetro de rol arbitrario. La mayoría de las veces, sólo tendrás que tratar con HumanMessage, AIMessage y SystemMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede pasar multiples mensajes para los modelos gpt-3.5-turbo y gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages)\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ir un paso más allá y generar terminaciones para múltiples conjuntos de mensajes utilizando generate. Esto devuelve un LLMResult con un parámetro de mensaje adicional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result\n",
    "# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes recuperar cosas como el uso de tokens a partir de este LLMResult:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output['token_usage']\n",
    "# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar a los LLMs, puedes hacer uso de plantillas utilizando un MessagePromptTemplate. Puedes construir un ChatPromptTemplate a partir de uno o más MessagePromptTemplates. Puede utilizar el format_prompt de ChatPromptTemplate - esto devuelve un PromptValue, que puede convertir en una cadena o en un objeto Message, dependiendo de si desea utilizar el valor formateado como entrada a un modelo llm o chat.\n",
    "\n",
    "Para mayor comodidad, existe un método from_template expuesto en la plantilla. Si fuera a utilizar esta plantilla, esto es lo que parecería:\n",
    "\n",
    "Traducción realizada con la versión gratuita del traductor www.DeepL.com/Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cadenas (Chains) con Modelos de Chat (Chat Models)**\n",
    "\n",
    "La LLMChain comentada en la sección anterior también puede utilizarse con modelos de chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "# -> \"J'aime programmer.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Agentes (Agents) con modelos de Chat (Chat Models)***\n",
    "\n",
    "Los agentes también se pueden utilizar con modelos de chat, puede inicializar uno utilizando AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION como tipo de agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find the answer to the first question and a calculator to solve the second question.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOlivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to use a calculator to find the answer to the second question.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"0.23^(age of Olivia Wilde's boyfriend)\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.9895113660064618e-22\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to find the age of Harry Styles to calculate the final answer.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Harry Styles age\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m29 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I can calculate the final answer.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"29^(0.23)\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.169459462491557\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the final answer.\n",
      "\n",
      "Final Answer: 2.169459462491557\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.169459462491557'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# os.environ['OPENAI_API_KEY']\n",
    "# os.environ['SERPAPI_API_KEY']\n",
    "\n",
    "# Primero, carguemos el modelo de lenguaje que vamos a utilizar para controlar el agente.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Despues, carguemos algunas herramientas a utilizar. Ten en cuenta que la herramienta `llm-math` utiliza un LLM, así que tenemos que pasarlo.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finalmente, inicialicemos un agente con las herramientas, el modelo de lenguaje y el tipo de agente que queremos utilizar.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Ahora probemos el agente!\n",
    "agent.run(\"¿Quién es el novio de Olivia Wilde? ¿Cuál es su edad actual a la 0.23 potencia?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Memoria: Añadir Estado a Cadenas y Agentes***\n",
    "\n",
    "Puedes utilizar Memory con cadenas y agentes inicializados con modelos de chat. La principal diferencia entre esto y la Memoria para LLMs es que en lugar de intentar condensar todos los mensajes anteriores en una cadena, podemos mantenerlos como su propio objeto de memoria único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! ¿Cómo estás? Soy una IA diseñada para conversar contigo. ¿En qué puedo ayudarte hoy?\n",
      "¡Genial! Me encanta tener conversaciones con humanos. ¿Hay algún tema en particular que te gustaría discutir o preguntar?\n",
      "¡Claro! Soy una IA diseñada para procesar y analizar grandes cantidades de datos. Mi objetivo es ayudar a las personas a obtener información útil y tomar decisiones informadas. Estoy programada para aprender y mejorar constantemente, por lo que siempre estoy buscando nuevas formas de mejorar mi capacidad para ayudar a las personas. ¿Hay algo más específico que te gustaría saber sobre mí?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"La siguiente es una conversación amistosa entre un humano y una IA. La IA es locuaz y proporciona muchos detalles específicos de su contexto. Si la IA no sabe la respuesta a una pregunta, dice la verdad.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "res = conversation.predict(input=\"¡Hola!\")\n",
    "print(res)\n",
    "\n",
    "\n",
    "res = conversation.predict(input=\"¡Todo bien! Sólo teniendo una conversación con una IA.\")\n",
    "print(res)\n",
    "\n",
    "res = conversation.predict(input=\"Háblame de ti.\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
