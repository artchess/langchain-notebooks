{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interfaz para chatmodels se basa en mensajes y no en texto sin formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI’s chat model supports multiple messages as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 53e929afe3002e3b85e316505dd1b33f in your message.).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ir un paso más allá y generar terminaciones para múltiples conjuntos de mensajes utilizando generate. Esto devuelve un LLMResult con un parámetro de mensaje adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 57,\n",
       "  'completion_tokens': 20,\n",
       "  'total_tokens': 77},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes hacer uso de las plantillas utilizando un MessagePromptTemplate. Puedes construir un ChatPromptTemplate a partir de uno o más MessagePromptTemplates. Puedes usar el format_prompt de ChatPromptTemplate - esto devuelve un PromptValue, que puedes convertir en una cadena o en un objeto Message, dependiendo de si quieres usar el valor formateado como entrada a un modelo llm o chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisieras construir el MessagePromptTemplate más directamente, podrías crear un PromptTemplate fuera y luego pasarlo, eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "    input_variables=[\"input_language\", \"output_language\"],\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede utilizar la LLMChain existente de una forma muy similar a la anterior: proporcione un indicador y un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transmisión (Streaming) es compatible con ChatOpenAI a través del manejo de devolución de llamada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Bubbles rising to the top\n",
      "A refreshing drink that never stops\n",
      "Clear and crisp, it's oh so pure\n",
      "Sparkling water, I can't ignore\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Verse 2:\n",
      "No sugar, no calories, just H2O\n",
      "A drink that's good for me, don't you know\n",
      "With lemon or lime, it's even better\n",
      "Sparkling water, you're my forever\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Bridge:\n",
      "I'll never get tired of your effervescence\n",
      "You're the perfect drink for any occasion\n",
      "Whether I'm at home or out on the town\n",
      "Sparkling water, you never let me down\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A taste so clean, it's simply divine\n",
      "You quench my thirst, you make me feel alive\n",
      "Sparkling water, you're my favorite vibe\n",
      "\n",
      "Outro:\n",
      "Sparkling water, you're the one for me\n",
      "A drink that's always refreshing and free\n",
      "I'll never give you up, you're my go-to\n",
      "Sparkling water, I love you."
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = chat([HumanMessage(content=\"Write me a song about sparkling water.\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
