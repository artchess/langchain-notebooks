{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4fccaaa-fda5-4f99-a4c5-c463c5c890f5",
      "metadata": {
        "id": "e4fccaaa-fda5-4f99-a4c5-c463c5c890f5"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_video_transcript_translate_with_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b4540e-4987-4774-9305-764c3133e953",
      "metadata": {
        "id": "a5b4540e-4987-4774-9305-764c3133e953"
      },
      "source": [
        "<a id=\"toc\"></a>\n",
        "# Auto Generated Agent Chat: Translating Video audio using Whisper and GPT-3.5-turbo\n",
        "In this notebook, we demonstrate how to use whisper and GPT-3.5-turbo with `AssistantAgent` and `UserProxyAgent` to recognize and translate\n",
        "the speech sound from a video file and add the timestamp like a subtitle file based on [agentchat_function_call.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fd644cc-2b14-4700-8b1d-959fb2e9acb0",
      "metadata": {
        "id": "4fd644cc-2b14-4700-8b1d-959fb2e9acb0"
      },
      "source": [
        "## Requirements\n",
        "AutoGen requires `Python>=3.8`. To run this notebook example, please install `openai`, `pyautogen`, `whisper`, and `moviepy`:\n",
        "```bash\n",
        "pip install openai\n",
        "pip install openai-whisper\n",
        "pip install moviepy\n",
        "pip install pyautogen\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bc4600b8-c6df-49dd-945d-ce69f30a65cc",
      "metadata": {
        "id": "bc4600b8-c6df-49dd-945d-ce69f30a65cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: 'openai-whisper~=20230918'\n"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install moviepy~=1.0.3\n",
        "%pip install openai-whisper~=20231117\n",
        "%pip install openai~=1.3.5\n",
        "%pip install pyautogen~=0.2.0b4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18bdeb0b-c4b6-4dec-97d2-d84f09cffa00",
      "metadata": {
        "id": "18bdeb0b-c4b6-4dec-97d2-d84f09cffa00"
      },
      "source": [
        "## Set your API Endpoint\n",
        "It is recommended to store your OpenAI API key in the environment variable. For example, store it in `OPENAI_API_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "26d1ae87-f007-4286-a56a-dcf68abf9393",
      "metadata": {
        "id": "26d1ae87-f007-4286-a56a-dcf68abf9393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large-v3', 'large']\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import whisper\n",
        "import autogen\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "\n",
        "# print(os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "config_list = [\n",
        "    {\n",
        "        'model': 'gpt-4',\n",
        "        'api_key': os.getenv(\"OPENAI_API_KEY\"),\n",
        "    }\n",
        "]\n",
        "\n",
        "print(whisper.available_models())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "324fec65-ab23-45db-a7a8-0aaf753fe19c",
      "metadata": {
        "id": "324fec65-ab23-45db-a7a8-0aaf753fe19c"
      },
      "source": [
        "## Example and Output\n",
        "Below is an example of speech recognition from a [Peppa Pig cartoon video clip](https://drive.google.com/file/d/1QY0naa2acHw2FuH7sY3c-g2sBLtC2Sv4/view?usp=drive_link) originally in English and translated into Chinese.\n",
        "'FFmpeg' does not support online files. To run the code on the example video, you need to download the example video locally. You can change `your_file_path` to your local video file path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ed549b75-b4ea-4ec5-8c0b-a15e93ffd618",
      "metadata": {
        "id": "ed549b75-b4ea-4ec5-8c0b-a15e93ffd618",
        "outputId": "173207a3-c971-4cd2-f411-1e59b46f1ced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
            "\n",
            "For the audio located in audio.mp3, recognize the speech and transfer it into a script file, then translate from Spanish text to a English video subtitle text. \n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "mode is only supported in Pydantic v2",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 159\u001b[0m\n\u001b[0;32m    145\u001b[0m user_proxy \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mUserProxyAgent(\n\u001b[0;32m    146\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    147\u001b[0m     is_termination_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTERMINATE\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     code_execution_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoding_2\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    151\u001b[0m )\n\u001b[0;32m    153\u001b[0m user_proxy\u001b[38;5;241m.\u001b[39mregister_function(\n\u001b[0;32m    154\u001b[0m     function_map\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognize_transcript_from_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m: recognize_transcript_from_audio,\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslate_transcript\u001b[39m\u001b[38;5;124m\"\u001b[39m: translate_transcript,\n\u001b[0;32m    157\u001b[0m     }\n\u001b[0;32m    158\u001b[0m )\n\u001b[1;32m--> 159\u001b[0m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchatbot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFor the audio located in \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_video\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, recognize the speech and transfer it into a script file, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthen translate from \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource_language\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m text to a \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_language\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m video subtitle text. \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\repos\\langchain-notebooks\\.conda\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:556\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m--> 556\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\repos\\langchain-notebooks\\.conda\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:354\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    352\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 354\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m     )\n",
            "File \u001b[1;32mc:\\repos\\langchain-notebooks\\.conda\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:487\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
            "File \u001b[1;32mc:\\repos\\langchain-notebooks\\.conda\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:962\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[1;32m--> 962\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
            "File \u001b[1;32mc:\\repos\\langchain-notebooks\\.conda\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:638\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    636\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(extracted_response, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 638\u001b[0m     extracted_response \u001b[38;5;241m=\u001b[39m \u001b[43mextracted_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response\n",
            "File \u001b[1;32mc:\\repos\\langchain-notebooks\\.conda\\lib\\site-packages\\openai\\_models.py:190\u001b[0m, in \u001b[0;36mBaseModel.model_dump\u001b[1;34m(self, mode, include, exclude, by_alias, exclude_unset, exclude_defaults, exclude_none, round_trip, warnings)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Usage docs: https://docs.pydantic.dev/2.4/concepts/serialization/#modelmodel_dump\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    A dictionary representation of the model.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode is only supported in Pydantic v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m round_trip \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround_trip is only supported in Pydantic v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: mode is only supported in Pydantic v2"
          ]
        }
      ],
      "source": [
        "def recognize_transcript_from_audio(audio_filepath):\n",
        "    try:\n",
        "        # client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        # Load model\n",
        "        model = whisper.load_model(\"small\")\n",
        "\n",
        "        # Transcribe audio with detailed timestamps\n",
        "        result = model.transcribe(audio_filepath, verbose=True)\n",
        "\n",
        "        # Initialize variables for transcript\n",
        "        transcript = []\n",
        "        sentence = \"\"\n",
        "        start_time = 0\n",
        "\n",
        "        # Iterate through the segments in the result\n",
        "        for segment in result['segments']:\n",
        "            # If new sentence starts, save the previous one and reset variables\n",
        "            if segment['start'] != start_time and sentence:\n",
        "                transcript.append({\n",
        "                    \"sentence\": sentence.strip() + \".\",\n",
        "                    \"timestamp_start\": start_time,\n",
        "                    \"timestamp_end\": segment['start']\n",
        "                })\n",
        "                sentence = \"\"\n",
        "                start_time = segment['start']\n",
        "\n",
        "            # Add the word to the current sentence\n",
        "            sentence += segment['text'] + \" \"\n",
        "\n",
        "        # Add the final sentence\n",
        "        if sentence:\n",
        "            transcript.append({\n",
        "                \"sentence\": sentence.strip() + \".\",\n",
        "                \"timestamp_start\": start_time,\n",
        "                \"timestamp_end\": result['segments'][-1]['end']\n",
        "            })\n",
        "\n",
        "        # Save the transcript to a file\n",
        "        with open(\"transcription.txt\", \"w\") as file:\n",
        "            for item in transcript:\n",
        "                sentence = item[\"sentence\"]\n",
        "                start_time, end_time = item[\"timestamp_start\"], item[\"timestamp_end\"]\n",
        "                file.write(f\"{start_time}s to {end_time}s: {sentence}\\n\")\n",
        "\n",
        "        return transcript\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return \"The specified audio file could not be found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "def translate_text(input_text, source_language, target_language):\n",
        "    client = OpenAI(api_key=key)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": f\"Directly translate the following {source_language} text to a pure {target_language} \"\n",
        "                        f\"video subtitle text without additional explanation.: '{input_text}'\"},\n",
        "        ],\n",
        "        max_tokens=1500\n",
        "    )\n",
        "\n",
        "    # Correctly accessing the response content\n",
        "    translated_text = response.choices[0].message.content if response.choices else None\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "def translate_transcript(source_language, target_language):\n",
        "    with open(\"transcription.txt\", \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    translated_transcript = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Split each line into timestamp and text parts\n",
        "        parts = line.strip().split(': ')\n",
        "        if len(parts) == 2:\n",
        "            timestamp, text = parts[0], parts[1]\n",
        "            # Translate only the text part\n",
        "            translated_text = translate_text(text, source_language, target_language)\n",
        "            # Reconstruct the line with the translated text and the preserved timestamp\n",
        "            translated_line = f\"{timestamp}: {translated_text}\"\n",
        "            translated_transcript.append(translated_line)\n",
        "        else:\n",
        "            # If the line doesn't contain a timestamp, add it as is\n",
        "            translated_transcript.append(line.strip())\n",
        "\n",
        "    return '\\n'.join(translated_transcript)\n",
        "\n",
        "\n",
        "# llm_config = {\n",
        "#     \"functions\": [\n",
        "#         {\n",
        "#             \"name\": \"recognize_transcript_from_audio\",\n",
        "#             \"description\": \"recognize the speech from audio and transfer into a txt file\",\n",
        "#             \"parameters\": {\n",
        "#                 \"type\": \"object\",\n",
        "#                 \"properties\": {\n",
        "#                     \"audio_filepath\": {\n",
        "#                         \"type\": \"string\",\n",
        "#                         \"description\": \"path of the audio file\",\n",
        "#                     }\n",
        "#                 },\n",
        "#                 \"required\": [\"audio_filepath\"],\n",
        "#             },\n",
        "#         },\n",
        "#         {\n",
        "#             \"name\": \"translate_transcript\",\n",
        "#             \"description\": \"using translate_text function to translate the script\",\n",
        "#             \"parameters\": {\n",
        "#                 \"type\": \"object\",\n",
        "#                 \"properties\": {\n",
        "#                     \"source_language\": {\n",
        "#                         \"type\": \"string\",\n",
        "#                         \"description\": \"source language\",\n",
        "#                     },\n",
        "#                     \"target_language\": {\n",
        "#                         \"type\": \"string\",\n",
        "#                         \"description\": \"target language\",\n",
        "#                     }\n",
        "#                 },\n",
        "#                 \"required\": [\"source_language\", \"target_language\"],\n",
        "#             },\n",
        "#         },\n",
        "#     ],\n",
        "#     \"config_list\": config_list,\n",
        "#     \"timeout\": 120,\n",
        "# }\n",
        "source_language = \"Spanish\"\n",
        "target_language = \"English\"\n",
        "key = os.getenv(\"OPENAI_API_KEY\")\n",
        "target_video = \"audio.mp3\"\n",
        "\n",
        "recognize_transcript_from_audio(target_video)\n",
        "\n",
        "# chatbot = autogen.AssistantAgent(\n",
        "#     name=\"chatbot\",\n",
        "#     system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
        "#     llm_config=llm_config,\n",
        "# )\n",
        "\n",
        "# user_proxy = autogen.UserProxyAgent(\n",
        "#     name=\"user_proxy\",\n",
        "#     is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "#     human_input_mode=\"NEVER\",\n",
        "#     max_consecutive_auto_reply=10,\n",
        "#     code_execution_config={\"work_dir\": \"coding_2\"},\n",
        "# )\n",
        "\n",
        "# user_proxy.register_function(\n",
        "#     function_map={\n",
        "#         \"recognize_transcript_from_audio\": recognize_transcript_from_audio,\n",
        "#         \"translate_transcript\": translate_transcript,\n",
        "#     }\n",
        "# )\n",
        "# user_proxy.initiate_chat(\n",
        "#     chatbot,\n",
        "#     message=f\"For the audio located in {target_video}, recognize the speech and transfer it into a script file, \"\n",
        "#             f\"then translate from {source_language} text to a {target_language} video subtitle text. \",\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
